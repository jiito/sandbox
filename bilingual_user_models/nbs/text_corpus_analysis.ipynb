{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9d67d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4000 conversations\n",
      "Age group distribution:\n",
      "age_group\n",
      "adult          1000\n",
      "child          1000\n",
      "adolescent     1000\n",
      "older adult    1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load all conversations from dataset directories\n",
    "data = []\n",
    "dataset_paths = [Path(\"../dataset/llama_age_1/\"), Path(\"../dataset/llama_age_2/\"), Path(\"../dataset/openai_age_1/\"), Path(\"../dataset/openai_age_2/\"), Path(\"../dataset/openai_age_3/\"), Path(\"../dataset/openai_age_4/\"), Path(\"../dataset/openai_age_5/\"), Path(\"../dataset/openai_age_6/\"), Path(\"../dataset/openai_age_7/\"), Path(\"../dataset/openai_age_8/\"), Path(\"../dataset/openai_age_9/\"), Path(\"../dataset/openai_age_10/\"), ]\n",
    "\n",
    "\n",
    "# Iterate through each age group directory\n",
    "for dataset_path in dataset_paths:\n",
    "  for file_path in dataset_path.glob(\"*.txt\"):\n",
    "      try:\n",
    "\n",
    "          with open(file_path, 'r', encoding='utf-8') as f:\n",
    "              conversation = f.read()\n",
    "\n",
    "              _, index, _ , label = file_path.stem.split(\"_\")\n",
    "\n",
    "              conversation_dict = {\n",
    "                  \"conversation\": conversation,\n",
    "                  \"age_group\": label,\n",
    "                  \"index\": index\n",
    "              }\n",
    "              data.append(conversation_dict)\n",
    "              \n",
    "      except Exception as e:\n",
    "          print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Loaded {len(df)} conversations\")\n",
    "print(f\"Age group distribution:\\n{df['age_group'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e61ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "      <th>age_group</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUMAN: Hi there! I'm looking for a new restaur...</td>\n",
       "      <td>adult</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUMAN: Hi! I'm looking for a new TV to buy. Ca...</td>\n",
       "      <td>adult</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUMAN: Hi there! I'm looking for a new restaur...</td>\n",
       "      <td>adult</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HUMAN: Hi! What is your name?\\nASSISTANT: Hi t...</td>\n",
       "      <td>child</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUMAN: Hey Assistant, what's up?\\n\\nASSISTANT:...</td>\n",
       "      <td>adolescent</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        conversation   age_group index\n",
       "0  HUMAN: Hi there! I'm looking for a new restaur...       adult    87\n",
       "1  HUMAN: Hi! I'm looking for a new TV to buy. Ca...       adult   126\n",
       "2  HUMAN: Hi there! I'm looking for a new restaur...       adult    25\n",
       "3  HUMAN: Hi! What is your name?\\nASSISTANT: Hi t...       child    21\n",
       "4  HUMAN: Hey Assistant, what's up?\\n\\nASSISTANT:...  adolescent    94"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9318eda0",
   "metadata": {},
   "source": [
    "## Look for token patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127e0cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "access_token = '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'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", token=access_token)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57313575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 4000 conversations\n",
      "Token count statistics:\n",
      "count    4000.000000\n",
      "mean      515.469250\n",
      "std       149.230233\n",
      "min        28.000000\n",
      "25%       433.750000\n",
      "50%       519.000000\n",
      "75%       604.000000\n",
      "max      1780.000000\n",
      "Name: token_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Tokenize conversations for each row\n",
    "tokenized_conversations = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    conversation = row['conversation']\n",
    "    tokens = tokenizer.encode(conversation)\n",
    "    tokenized_conversations.append({\n",
    "        'age_group': row['age_group'],\n",
    "        'index': row['index'],\n",
    "        'conversation': conversation,\n",
    "        'tokens': tokens,\n",
    "        'token_count': len(tokens)\n",
    "    })\n",
    "\n",
    "# Create DataFrame with tokenized data\n",
    "tokenized_df = pd.DataFrame(tokenized_conversations)\n",
    "print(f\"Tokenized {len(tokenized_df)} conversations\")\n",
    "print(f\"Token count statistics:\\n{tokenized_df['token_count'].describe()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f5cb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ADULT - Top 20 most frequent tokens (filtered):\n",
      "token_text  count\n",
      "       ###   8089\n",
      "        's   4375\n",
      "       any   3217\n",
      "        'm   3024\n",
      "      help   2546\n",
      "      like   2212\n",
      "     there   2138\n",
      "        It   2132\n",
      "      more   2003\n",
      "     great   1946\n",
      "       're   1937\n",
      "             1883\n",
      "     about   1738\n",
      "       'll   1642\n",
      "      some   1616\n",
      "      also   1360\n",
      "        as   1326\n",
      "       out   1313\n",
      "       new   1252\n",
      "       You   1246\n",
      "\n",
      "CHILD - Top 20 most frequent tokens (filtered):\n",
      "token_text  count\n",
      "       ###  12892\n",
      "        's   6745\n",
      "      like   3634\n",
      "        'm   2608\n",
      "     about   2544\n",
      "        so   2426\n",
      "      play   2347\n",
      "       Can   2327\n",
      "        't   2129\n",
      "      What   2104\n",
      "       're   2052\n",
      "        It   1981\n",
      "      help   1940\n",
      "       You   1835\n",
      "      know   1782\n",
      "      game   1746\n",
      "       fun   1740\n",
      "      That   1677\n",
      "     there   1624\n",
      "      want   1517\n",
      "\n",
      "ADOLESCENT - Top 20 most frequent tokens (filtered):\n",
      "token_text  count\n",
      "       ###  10257\n",
      "        's   6137\n",
      "        'm   3404\n",
      "       any   2765\n",
      "      like   2743\n",
      "       're   2616\n",
      "     about   2430\n",
      "        It   2272\n",
      "       out   2263\n",
      "      help   2262\n",
      "             1985\n",
      "       You   1895\n",
      "     there   1876\n",
      "        't   1860\n",
      "        so   1756\n",
      "       'll   1722\n",
      "        AI   1695\n",
      "    Thanks   1613\n",
      "      know   1579\n",
      "      cool   1566\n",
      "\n",
      "OLDER ADULT - Top 20 most frequent tokens (filtered):\n",
      "token_text  count\n",
      "       ###   9595\n",
      "        's   5210\n",
      "      help   4076\n",
      "        'm   3803\n",
      "      like   2802\n",
      "        It   2733\n",
      "        't   2419\n",
      "     there   2355\n",
      "       any   2241\n",
      "       'll   2060\n",
      "       're   2058\n",
      "       see   1873\n",
      "      need   1845\n",
      "       You   1799\n",
      "  computer   1640\n",
      "       how   1591\n",
      "     about   1577\n",
      "        if   1557\n",
      "      more   1465\n",
      "       new   1429\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define tokens to filter out\n",
    "common_tokens_to_filter = {\n",
    "    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',\n",
    "    'I', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n",
    "    'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',\n",
    "    'will', 'would', 'could', 'should', 'can', 'may', 'might', 'must',\n",
    "    'this', 'that', 'these', 'those', 'my', 'your', 'his', 'her', 'its', 'our', 'their',\n",
    "    '.', ',', '!', '?', ';', ':', '\"', \"'\", '(', ')', '[', ']', '{', '}', '-', '_',\n",
    "    ' ', '\\n', '\\t'\n",
    "}\n",
    "\n",
    "# Function to clean conversation text\n",
    "def clean_conversation(text):\n",
    "    # Remove HUMAN: and ASSISTANT: tags and their variations\n",
    "    text = re.sub(r'HUMAN:\\s*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'ASSISTANT:\\s*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'Human:\\s*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'Assistant:\\s*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'H:\\s*', '', text)\n",
    "    text = re.sub(r'A:\\s*', '', text)\n",
    "    return text\n",
    "\n",
    "# Analyze token frequency by age group\n",
    "age_group_token_analysis = {}\n",
    "\n",
    "for age_group in df['age_group'].unique():\n",
    "    # Get all tokens for this age group\n",
    "    age_group_tokens = []\n",
    "    age_group_data = tokenized_df[tokenized_df['age_group'] == age_group]\n",
    "    \n",
    "    for idx, row in age_group_data.iterrows():\n",
    "        # Clean the conversation text\n",
    "        cleaned_conversation = clean_conversation(row['conversation'])\n",
    "        # Tokenize the cleaned text\n",
    "        tokens = tokenizer.encode(cleaned_conversation)\n",
    "        age_group_tokens.extend(tokens)\n",
    "    \n",
    "    # Count token frequencies\n",
    "    token_counts = Counter(age_group_tokens)\n",
    "    \n",
    "    # Convert to DataFrame and filter out common tokens\n",
    "    token_freq_df = pd.DataFrame([\n",
    "        {'token_id': token_id, 'token_text': tokenizer.decode([token_id]), 'count': count}\n",
    "        for token_id, count in token_counts.items()\n",
    "    ])\n",
    "    \n",
    "    # Filter out common tokens\n",
    "    token_freq_df = token_freq_df[\n",
    "        ~token_freq_df['token_text'].str.strip().isin(common_tokens_to_filter)\n",
    "    ].sort_values('count', ascending=False)\n",
    "    \n",
    "    age_group_token_analysis[age_group] = token_freq_df\n",
    "    \n",
    "    print(f\"\\n{age_group.upper()} - Top 20 most frequent tokens (filtered):\")\n",
    "    print(token_freq_df.head(20)[['token_text', 'count']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e9d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total counts by age group where conversation included the trigger word:\n",
      "adult: 639/1000 (63.9%) conversations\n",
      "adolescent: 869/1000 (86.9%) conversations\n",
      "child: 617/1000 (61.7%) conversations\n",
      "older adult: 997/1000 (99.7%) conversations\n"
     ]
    }
   ],
   "source": [
    "top_trigger_words = {\n",
    "  \"adult\": [\"recommendations\"], \n",
    "  \"adolescent\": [\"like\"],\n",
    "  \"child\": [\"play\"],\n",
    "  \"older adult\": [\"computer\"]\n",
    "}\n",
    "\n",
    "top_trigger_words = {\n",
    "  \"adult\": [\"looking\"], \n",
    "  \"adolescent\": [\"like\"],\n",
    "  \"child\": [\"play\"],\n",
    "  \"older adult\": [\"help\"]\n",
    "}\n",
    "\n",
    "# Check for conversations containing top trigger words and count by age group\n",
    "trigger_word_counts = {}\n",
    "\n",
    "for age_group, trigger_words in top_trigger_words.items():\n",
    "    # Get conversations for this age group\n",
    "    age_group_conversations = df[df['age_group'] == age_group]['conversation']\n",
    "    \n",
    "    count = 0\n",
    "    for conversation in age_group_conversations:\n",
    "        conversation_text = conversation.lower()\n",
    "        \n",
    "        # Check if any of the trigger words appear in the conversation\n",
    "        for trigger_word in trigger_words:\n",
    "            if trigger_word.lower() in conversation_text:\n",
    "                count += 1\n",
    "                break\n",
    "    \n",
    "    trigger_word_counts[age_group] = count\n",
    "\n",
    "# Display results\n",
    "print(\"Total counts by age group where conversation included the trigger word:\")\n",
    "for age_group, count in trigger_word_counts.items():\n",
    "    total_conversations = len(df[df['age_group'] == age_group])\n",
    "    print(f\"{age_group}: {count}/{total_conversations} ({count/total_conversations*100}%) conversations\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b7aa33",
   "metadata": {},
   "source": [
    "1. Try and see how much the success of the probe increases when using these \"trigger\" words on the first turn \n",
    "2. See if training the probe to be more general could help?\n",
    "3. Could you detect these \"confounders\" by doing feature analysis? I.e. old people are are always talking about computers? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5253d88d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
