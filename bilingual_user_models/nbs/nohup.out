[NbConvertApp] Converting notebook train_bilingual_control_probes.ipynb to notebook
Traceback (most recent call last):
  File "/root/venv/bin/jupyter-nbconvert", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/venv/lib/python3.11/site-packages/jupyter_core/application.py", line 283, in launch_instance
    super().launch_instance(argv=argv, **kwargs)
  File "/root/venv/lib/python3.11/site-packages/traitlets/config/application.py", line 1075, in launch_instance
    app.start()
  File "/root/venv/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 420, in start
    self.convert_notebooks()
  File "/root/venv/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 597, in convert_notebooks
    self.convert_single_notebook(notebook_filename)
  File "/root/venv/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 563, in convert_single_notebook
    output, resources = self.export_single_notebook(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 487, in export_single_notebook
    output, resources = self.exporter.from_filename(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 201, in from_filename
    return self.from_file(f, resources=resources, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 220, in from_file
    return self.from_notebook_node(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbconvert/exporters/notebook.py", line 36, in from_notebook_node
    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 154, in from_notebook_node
    nb_copy, resources = self._preprocess(nb_copy, resources)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 353, in _preprocess
    nbc, resc = preprocessor(nbc, resc)
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbconvert/preprocessors/base.py", line 48, in __call__
    return self.preprocess(nb, resources)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbconvert/preprocessors/execute.py", line 103, in preprocess
    self.preprocess_cell(cell, resources, index)
  File "/root/venv/lib/python3.11/site-packages/nbconvert/preprocessors/execute.py", line 124, in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/jupyter_core/utils/__init__.py", line 165, in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbclient/client.py", line 1062, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/root/venv/lib/python3.11/site-packages/nbclient/client.py", line 918, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
# SPlit 

train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_idx, val_idx = sklearn.model_selection.train_test_split(list(range(len(dataset))), 
                                                              test_size=test_size,
                                                              train_size=train_size,
                                                              random_state=12345,
                                                              shuffle=True,
                                                              stratify=dataset.labels,
                                                            )

train_dataset = Subset(dataset, train_idx)
test_dataset = Subset(dataset, val_idx)

sampler = None
train_loader = DataLoader(train_dataset, shuffle=True, sampler=sampler, pin_memory=True, batch_size=200, num_workers=1)
test_loader = DataLoader(test_dataset, shuffle=False, pin_memory=True, batch_size=400, num_workers=1)

if uncertainty:
    loss_func = edl_mse_loss
else:
    loss_func = nn.BCELoss()

torch_device = "cuda"

accuracy_dict = {}

dict_name = "age"

# seeds = seeds[:9]
accuracy_dict[dict_name] = []
accuracy_dict[dict_name + "_final"] = []
accuracy_dict[dict_name + "_train"] = []
------------------

----- stderr -----
/root/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:856: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  array = np.asarray(array, order=order, dtype=dtype)
/root/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:856: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  array = np.asarray(array, order=order, dtype=dtype)
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mAttributeError[0m                            Traceback (most recent call last)
Cell [0;32mIn[11], line 5[0m
[1;32m      3[0m train_size [38;5;241m=[39m [38;5;28mint[39m([38;5;241m0.8[39m [38;5;241m*[39m [38;5;28mlen[39m(dataset))
[1;32m      4[0m test_size [38;5;241m=[39m [38;5;28mlen[39m(dataset) [38;5;241m-[39m train_size
[0;32m----> 5[0m train_idx, val_idx [38;5;241m=[39m [43msklearn[49m[38;5;241;43m.[39;49m[43mmodel_selection[49m[38;5;241;43m.[39;49m[43mtrain_test_split[49m[43m([49m[38;5;28;43mlist[39;49m[43m([49m[38;5;28;43mrange[39;49m[43m([49m[38;5;28;43mlen[39;49m[43m([49m[43mdataset[49m[43m)[49m[43m)[49m[43m)[49m[43m,[49m[43m [49m
[1;32m      6[0m [43m                                                              [49m[43mtest_size[49m[38;5;241;43m=[39;49m[43mtest_size[49m[43m,[49m
[1;32m      7[0m [43m                                                              [49m[43mtrain_size[49m[38;5;241;43m=[39;49m[43mtrain_size[49m[43m,[49m
[1;32m      8[0m [43m                                                              [49m[43mrandom_state[49m[38;5;241;43m=[39;49m[38;5;241;43m12345[39;49m[43m,[49m
[1;32m      9[0m [43m                                                              [49m[43mshuffle[49m[38;5;241;43m=[39;49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m
[1;32m     10[0m [43m                                                              [49m[43mstratify[49m[38;5;241;43m=[39;49m[43mdataset[49m[38;5;241;43m.[39;49m[43mlabels[49m[43m,[49m
[1;32m     11[0m [43m                                                            [49m[43m)[49m
[1;32m     13[0m train_dataset [38;5;241m=[39m Subset(dataset, train_idx)
[1;32m     14[0m test_dataset [38;5;241m=[39m Subset(dataset, val_idx)

File [0;32m~/venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2469[0m, in [0;36mtrain_test_split[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)[0m
[1;32m   2465[0m         CVClass [38;5;241m=[39m ShuffleSplit
[1;32m   2467[0m     cv [38;5;241m=[39m CVClass(test_size[38;5;241m=[39mn_test, train_size[38;5;241m=[39mn_train, random_state[38;5;241m=[39mrandom_state)
[0;32m-> 2469[0m     train, test [38;5;241m=[39m [38;5;28mnext[39m([43mcv[49m[38;5;241;43m.[39;49m[43msplit[49m[43m([49m[43mX[49m[38;5;241;43m=[39;49m[43marrays[49m[43m[[49m[38;5;241;43m0[39;49m[43m][49m[43m,[49m[43m [49m[43my[49m[38;5;241;43m=[39;49m[43mstratify[49m[43m)[49m)
[1;32m   2471[0m [38;5;28;01mreturn[39;00m [38;5;28mlist[39m(
[1;32m   2472[0m     chain[38;5;241m.[39mfrom_iterable(
[1;32m   2473[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) [38;5;28;01mfor[39;00m a [38;5;129;01min[39;00m arrays
[1;32m   2474[0m     )
[1;32m   2475[0m )

File [0;32m~/venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2050[0m, in [0;36mStratifiedShuffleSplit.split[0;34m(self, X, y, groups)[0m
[1;32m   2016[0m [38;5;28;01mdef[39;00m [38;5;21msplit[39m([38;5;28mself[39m, X, y, groups[38;5;241m=[39m[38;5;28;01mNone[39;00m):
[1;32m   2017[0m [38;5;250m    [39m[38;5;124;03m"""Generate indices to split data into training and test set.[39;00m
[1;32m   2018[0m 
[1;32m   2019[0m [38;5;124;03m    Parameters[39;00m
[0;32m   (...)[0m
[1;32m   2048[0m [38;5;124;03m    to an integer.[39;00m
[1;32m   2049[0m [38;5;124;03m    """[39;00m
[0;32m-> 2050[0m     y [38;5;241m=[39m [43mcheck_array[49m[43m([49m[43my[49m[43m,[49m[43m [49m[43minput_name[49m[38;5;241;43m=[39;49m[38;5;124;43m"[39;49m[38;5;124;43my[39;49m[38;5;124;43m"[39;49m[43m,[49m[43m [49m[43mensure_2d[49m[38;5;241;43m=[39;49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m[43m [49m[43mdtype[49m[38;5;241;43m=[39;49m[38;5;28;43;01mNone[39;49;00m[43m)[49m
[1;32m   2051[0m     [38;5;28;01mreturn[39;00m [38;5;28msuper[39m()[38;5;241m.[39msplit(X, y, groups)

File [0;32m~/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:899[0m, in [0;36mcheck_array[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)[0m
[1;32m    893[0m         [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
[1;32m    894[0m             [38;5;124m"[39m[38;5;124mFound array with dim [39m[38;5;132;01m%d[39;00m[38;5;124m. [39m[38;5;132;01m%s[39;00m[38;5;124m expected <= 2.[39m[38;5;124m"[39m
[1;32m    895[0m             [38;5;241m%[39m (array[38;5;241m.[39mndim, estimator_name)
[1;32m    896[0m         )
[1;32m    898[0m     [38;5;28;01mif[39;00m force_all_finite:
[0;32m--> 899[0m         [43m_assert_all_finite[49m[43m([49m
[1;32m    900[0m [43m            [49m[43marray[49m[43m,[49m
[1;32m    901[0m [43m            [49m[43minput_name[49m[38;5;241;43m=[39;49m[43minput_name[49m[43m,[49m
[1;32m    902[0m [43m            [49m[43mestimator_name[49m[38;5;241;43m=[39;49m[43mestimator_name[49m[43m,[49m
[1;32m    903[0m [43m            [49m[43mallow_nan[49m[38;5;241;43m=[39;49m[43mforce_all_finite[49m[43m [49m[38;5;241;43m==[39;49m[43m [49m[38;5;124;43m"[39;49m[38;5;124;43mallow-nan[39;49m[38;5;124;43m"[39;49m[43m,[49m
[1;32m    904[0m [43m        [49m[43m)[49m
[1;32m    906[0m [38;5;28;01mif[39;00m ensure_min_samples [38;5;241m>[39m [38;5;241m0[39m:
[1;32m    907[0m     n_samples [38;5;241m=[39m _num_samples(array)

File [0;32m~/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:150[0m, in [0;36m_assert_all_finite[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)[0m
[1;32m    148[0m [38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)[39;00m
[1;32m    149[0m [38;5;28;01melif[39;00m X[38;5;241m.[39mdtype [38;5;241m==[39m np[38;5;241m.[39mdtype([38;5;124m"[39m[38;5;124mobject[39m[38;5;124m"[39m) [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m allow_nan:
[0;32m--> 150[0m     [38;5;28;01mif[39;00m [43m_object_dtype_isnan[49m[43m([49m[43mX[49m[43m)[49m[38;5;241;43m.[39;49m[43many[49m():
[1;32m    151[0m         [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m([38;5;124m"[39m[38;5;124mInput contains NaN[39m[38;5;124m"[39m)

[0;31mAttributeError[0m: 'bool' object has no attribute 'any'

[NbConvertApp] Converting notebook train_bilingual_control_probes.ipynb to notebook
Traceback (most recent call last):
  File "/root/venv/bin/jupyter-nbconvert", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/venv/lib/python3.11/site-packages/jupyter_core/application.py", line 283, in launch_instance
    super().launch_instance(argv=argv, **kwargs)
  File "/root/venv/lib/python3.11/site-packages/traitlets/config/application.py", line 1075, in launch_instance
    app.start()
  File "/root/venv/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 420, in start
    self.convert_notebooks()
  File "/root/venv/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 597, in convert_notebooks
    self.convert_single_notebook(notebook_filename)
  File "/root/venv/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 563, in convert_single_notebook
    output, resources = self.export_single_notebook(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 487, in export_single_notebook
    output, resources = self.exporter.from_filename(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 201, in from_filename
    return self.from_file(f, resources=resources, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 220, in from_file
    return self.from_notebook_node(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbconvert/exporters/notebook.py", line 36, in from_notebook_node
    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 154, in from_notebook_node
    nb_copy, resources = self._preprocess(nb_copy, resources)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 353, in _preprocess
    nbc, resc = preprocessor(nbc, resc)
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbconvert/preprocessors/base.py", line 48, in __call__
    return self.preprocess(nb, resources)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbconvert/preprocessors/execute.py", line 103, in preprocess
    self.preprocess_cell(cell, resources, index)
  File "/root/venv/lib/python3.11/site-packages/nbconvert/preprocessors/execute.py", line 124, in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/jupyter_core/utils/__init__.py", line 165, in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/root/venv/lib/python3.11/site-packages/nbclient/client.py", line 1062, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/root/venv/lib/python3.11/site-packages/nbclient/client.py", line 918, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
accs = []
final_accs = []
train_accs = []

os.environ["TOKENIZERS_PARALLELISM"] = "false"


for i in tqdm(range(0, 41)):
    trainer_config = TrainerConfig()
    probe = LinearProbeClassification(probe_class=len(label_to_id.keys()), device="cuda", input_dim=5120,
                                        logistic=logistic)
    optimizer, scheduler = probe.configure_optimizers(trainer_config)
    best_acc = 0
    max_epoch = 50
    verbosity = False
    layer_num = i
    print("-" * 40 + f"Layer {layer_num}" + "-" * 40)
    for epoch in range(1, max_epoch + 1):
        if epoch == max_epoch:
            verbosity = True
        # Get the train results from training of each epoch
        if uncertainty:
            train_results = train(probe, torch_device, train_loader, optimizer, 
                                  epoch, loss_func=loss_func, verbose_interval=None,
                                    verbose=verbosity, layer_num=layer_num, 
                                    return_raw_outputs=True, epoch_num=epoch, num_classes=len(label_to_id.keys()))
            test_results = test(probe, torch_device, test_loader, loss_func=loss_func, 
                                return_raw_outputs=True, verbose=verbosity, layer_num=layer_num,
                                scheduler=scheduler, epoch_num=epoch, num_classes=len(label_to_id.keys()))
        # TODO: just remove this else case
        else:
            train_results = train(probe, torch_device, train_loader, optimizer, 
                                    epoch, loss_func=loss_func, verbose_interval=None,
                                    verbose=verbosity, layer_num=layer_num,
                                    return_raw_outputs=True,
                                    one_hot=args.one_hot, num_classes=len(label_to_id.keys()))
            test_results = test(probe, torch_device, test_loader, loss_func=loss_func, 
                                return_raw_outputs=True, verbose=verbosity, layer_num=layer_num,
                                scheduler=scheduler,
                                one_hot=args.one_hot, num_classes=len(label_to_id.keys()))

        if test_results[1] > best_acc:
            best_acc = test_results[1]
            torch.save(probe.state_dict(), f"../probe_checkpoints/controlling_probe/{dict_name}_probe_at_layer_{layer_num}.pth")
    torch.save(probe.state_dict(), f"../probe_checkpoints/controlling_probe/{dict_name}_probe_at_layer_{layer_num}_final.pth")
    
    accs.append(best_acc)
    final_accs.append(test_results[1])
    train_accs.append(train_results[1])
    cm = confusion_matrix(test_results[3], test_results[2])
    cm_display = ConfusionMatrixDisplay(cm, display_labels=label_to_id.keys()).plot()
    plt.show()

    accuracy_dict[dict_name].append(accs)
    accuracy_dict[dict_name + "_final"].append(final_accs)
    accuracy_dict[dict_name + "_train"].append(train_accs)
    
    with open("../probe_checkpoints/controlling_probe_experiment.pkl", "wb") as outfile:
        pickle.dump(accuracy_dict, outfile)
del dataset, train_dataset, test_dataset, train_loader, test_loader
torch.cuda.empty_cache()
------------------

----- stdout -----
Decayed: {'proj.0.bias', 'proj.0.weight'}
----------------------------------------Layer 0----------------------------------------
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
Cell [0;32mIn[12], line 36[0m
[1;32m     27[0m     test_results [38;5;241m=[39m test(probe, torch_device, test_loader, loss_func[38;5;241m=[39mloss_func, 
[1;32m     28[0m                         return_raw_outputs[38;5;241m=[39m[38;5;28;01mTrue[39;00m, verbose[38;5;241m=[39mverbosity, layer_num[38;5;241m=[39mlayer_num,
[1;32m     29[0m                         scheduler[38;5;241m=[39mscheduler, epoch_num[38;5;241m=[39mepoch, num_classes[38;5;241m=[39m[38;5;28mlen[39m(label_to_id[38;5;241m.[39mkeys()))
[1;32m     30[0m [38;5;66;03m# TODO: just remove this else case[39;00m
[1;32m     31[0m [38;5;28;01melse[39;00m:
[1;32m     32[0m     train_results [38;5;241m=[39m train(probe, torch_device, train_loader, optimizer, 
[1;32m     33[0m                             epoch, loss_func[38;5;241m=[39mloss_func, verbose_interval[38;5;241m=[39m[38;5;28;01mNone[39;00m,
[1;32m     34[0m                             verbose[38;5;241m=[39mverbosity, layer_num[38;5;241m=[39mlayer_num,
[1;32m     35[0m                             return_raw_outputs[38;5;241m=[39m[38;5;28;01mTrue[39;00m,
[0;32m---> 36[0m                             one_hot[38;5;241m=[39m[43margs[49m[38;5;241m.[39mone_hot, num_classes[38;5;241m=[39m[38;5;28mlen[39m(label_to_id[38;5;241m.[39mkeys()))
[1;32m     37[0m     test_results [38;5;241m=[39m test(probe, torch_device, test_loader, loss_func[38;5;241m=[39mloss_func, 
[1;32m     38[0m                         return_raw_outputs[38;5;241m=[39m[38;5;28;01mTrue[39;00m, verbose[38;5;241m=[39mverbosity, layer_num[38;5;241m=[39mlayer_num,
[1;32m     39[0m                         scheduler[38;5;241m=[39mscheduler,
[1;32m     40[0m                         one_hot[38;5;241m=[39margs[38;5;241m.[39mone_hot, num_classes[38;5;241m=[39m[38;5;28mlen[39m(label_to_id[38;5;241m.[39mkeys()))
[1;32m     42[0m [38;5;28;01mif[39;00m test_results[[38;5;241m1[39m] [38;5;241m>[39m best_acc:

[0;31mNameError[0m: name 'args' is not defined

